{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "os.environ['KERAS_BACKEND']        = 'tensorflow'\n",
    "os.environ['CUDA_DEVICE_ORDER']    = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "tf.Session(config = config)\n",
    "\n",
    "import keras.backend as K\n",
    "K.set_image_data_format('channels_last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv2D, Activation, Dropout, Flatten, Dense, Input, MaxPooling2D, AveragePooling2D, BatchNormalization\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.merge import Concatenate, add\n",
    "from keras.utils import multi_gpu_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_conv_init = RandomNormal(0, 0.02)\n",
    "_gamma_init = RandomNormal(1., 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CONV(f, *a, **k):\n",
    "    return Conv2D(f, kernel_initializer = _conv_init, *a, **k)\n",
    "\n",
    "def NORM():\n",
    "    return BatchNormalization(momentum=0.9, axis=-1, epsilon=1.01e-5, gamma_initializer = _gamma_init)\n",
    "\n",
    "def POOL(*a, **k):\n",
    "    return MaxPooling2D(*a, **k)\n",
    "\n",
    "def BLOCK(_input, _nb_feature):\n",
    "    _nb_inception = _nb_feature // 4\n",
    "    \n",
    "    _layer_1 = CONV(_nb_inception, (1, 1), strides = 1, padding = 'same') (_input)\n",
    "    _layer_1 = NORM() (_layer_1, training = 1)\n",
    "    _layer_1 = Activation('relu') (_layer_1)\n",
    "\n",
    "    _layer_2 = CONV(_nb_inception, (1, 3), strides = 1, padding = 'same') (_input)\n",
    "    _layer_2 = NORM() (_layer_2, training = 1)\n",
    "    _layer_2 = Activation('relu') (_layer_2)\n",
    "\n",
    "    _layer_3 = CONV(_nb_inception, (3, 1), strides = 1, padding = 'same') (_input)\n",
    "    _layer_3 = NORM() (_layer_3, training = 1)\n",
    "    _layer_3 = Activation('relu') (_layer_3)\n",
    "\n",
    "    _layer_4 = CONV(_nb_inception, (3, 3), strides = 1, padding = 'same') (_input)\n",
    "    _layer_4 = NORM() (_layer_4, training = 1)\n",
    "    _layer_4 = Activation('relu') (_layer_4)\n",
    "    \n",
    "    _layer = Concatenate(-1)([_layer_1, _layer_2, _layer_3, _layer_4])\n",
    "    _layer = CONV(_nb_inception * 2, (1, 1), strides = 1, padding = 'same') (_layer)\n",
    "    _layer = Activation('relu') (_layer)\n",
    "    _layer = CONV(_nb_feature, (3, 3), strides = 1, padding = 'same') (_layer)\n",
    "    _layer = Activation('relu') (_layer)\n",
    "    \n",
    "    return _layer\n",
    "\n",
    "def CoFFee(_isize, _nc_in, _nc_out, _growth_rate = 64, _nb_block = 3):\n",
    "\n",
    "    _input = Input(shape = (_isize, _isize, _nc_in))\n",
    "\n",
    "    _nb_feature = 64\n",
    "    _layer = CONV(_nb_feature, (3, 3), strides = 1, padding = 'same') (_input)\n",
    "    _layer = Activation('relu') (_layer)\n",
    "    \n",
    "    for _i in range(_nb_block):\n",
    "        \n",
    "        _ip = _layer\n",
    "        _nb_feature += _growth_rate\n",
    "        _layer = BLOCK(_ip, _nb_feature)\n",
    "\n",
    "        _ip = Concatenate(-1) ([_ip, _layer])\n",
    "        _nb_feature += _growth_rate\n",
    "        _layer = BLOCK(_ip, _nb_feature)\n",
    "\n",
    "        _ip = Concatenate(-1) ([_ip, _layer])\n",
    "        _nb_feature += _growth_rate\n",
    "        _layer = BLOCK(_ip, _nb_feature)\n",
    "        \n",
    "        _layer = POOL(pool_size = (3, 3), strides = 2, padding = 'same') (_layer)\n",
    "        \n",
    "    _shape = _layer.get_shape().as_list()\n",
    "    \n",
    "    _layer = AveragePooling2D(pool_size = (_shape[1], _shape[2]), strides = (_shape[1], _shape[2]), padding = 'same') (_layer)\n",
    "\n",
    "    _layer = Flatten() (_layer)\n",
    "    \n",
    "    _layer = Dense(units = _nc_out) (_layer)\n",
    "    _layer = ACtivation('softmax') (_layer)\n",
    "    \n",
    "    return Model(inputs = _input, outputs = _layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_isize = 256\n",
    "_nb_block = 3\n",
    "_nc_in = 1\n",
    "_nc_out = 2\n",
    "_bsize = 4\n",
    "_growth_rate = 64\n",
    "_max_epoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_model = CoFFee(_isize, _nc_in, _nc_out, _growth_rate, _nb_block)\n",
    "_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_categorical_crossentropy(weights):\n",
    "    weights = K.variable(weights)\n",
    "    def loss(y_true, y_pred):\n",
    "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        loss = y_true * K.log(y_pred) * weights\n",
    "        loss = -K.sum(loss, -1)\n",
    "        return loss\n",
    "    return loss\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "_adam = Adam(lr = 2e-4)\n",
    "_weights = np.array([1.0,1.0])\n",
    "_model.compile(loss=weighted_categorical_crossentropy(_weights), optimizer=_adam, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "_path_training = ''\n",
    "_path_test = ''\n",
    "_color_mode = 'grayscale'\n",
    "\n",
    "_generator_training = ImageDataGenerator(rescale = 1./255., validation_split = 0.1)\n",
    "_dataset_training = _generator_training.flow_from_directory(_path_train,\n",
    "                                                            target_size = (_isize, _isize),\n",
    "                                                            batch_size = _bsize,\n",
    "                                                            class_mode = 'categorical',\n",
    "                                                            color_mode = _color_mode,\n",
    "                                                            subset = 'training')\n",
    "\n",
    "_dataset_validation = _generator_training.flow_from_directory(_path_grain,\n",
    "                                                              target_size = (_isize, _isize),\n",
    "                                                              batch_size = _bsize,\n",
    "                                                              class_mode = 'categorical',\n",
    "                                                              color_mode = _color_mode,\n",
    "                                                              subset = 'validation')\n",
    "\n",
    "_generator_testing = ImageDataGenerator(rescale = 1./255.)\n",
    "_dataset_testing = _generator_testing.flow_from_directory(_path_test,\n",
    "                                                          target_size = (_isize, _isize),\n",
    "                                                          batch_size = _bsize,\n",
    "                                                          class_mode = 'categorical',\n",
    "                                                          color_mode = _color_mode,\n",
    "                                                          shuffle = False)\n",
    "\n",
    "\n",
    "\n",
    "_nb_batch_training = int(_dataset_training.n / _bsize)\n",
    "_nb_batch_validation = int(_dataset_validation.n / _bsize)\n",
    "_nb_batch_testing = int(_dataset_testing.n / _bsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CUSTOM_HISTORY(keras.callbacks.Callback):\n",
    "    def init(self):\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        self.train_acc = []\n",
    "        self.val_acc = []\n",
    "    def on_epoch_end(self, batch, logs = {}):\n",
    "        self.train_loss.append(logs.get('loss'))\n",
    "        self.val_loss.append(logs.get('val_loss'))\n",
    "        self.train_acc.append(logs.get('acc'))\n",
    "        self.val_acc.append(logs.get('val_acc'))\n",
    "CUSTOM_CALLBACK = CUSTOM_HISTORY()\n",
    "CUSTOM_CALLBACK.init()\n",
    "\n",
    "def STATISTICS(_labels, _results):\n",
    "    _h, _f, _m, _n = 0, 0, 0, 0\n",
    "\n",
    "    assert len(_labels) == (_results.shape[0])\n",
    "    \n",
    "    for _i in range(len(_labels)):\n",
    "        \n",
    "        _label = _labels[_i]\n",
    "        _result = np.argmax(_results[_i])\n",
    "        \n",
    "        if _label == 1 and _result == 1 :\n",
    "            _h += 1\n",
    "        elif _label == 1 and _result == 0 :\n",
    "            _m += 1\n",
    "        elif _label == 0 and _result == 1 :\n",
    "            _f += 1\n",
    "        elif _label == 0 and _result == 0 :\n",
    "            _n += 1\n",
    "        else :\n",
    "            pass\n",
    "        \n",
    "    _acc = (_h + _n)/(_h + _m + _f + _n)\n",
    "    _pod = _h/(_h + _m)\n",
    "    _far = _f/(_h + _f)\n",
    "    _csi = _h/(_h + _m + _f)\n",
    "    _hss = (2 * (_h * _n - _f * _m)) / ((_h + _m) * (_m + _n) + (_h + _f) * (_f + _n))\n",
    "    _tss = (_h * N - _f * _m)/ ((_h + _m) * (_f + _n))\n",
    "    \n",
    "    return _acc, _pod, _csi, _far, _hss, _tss\n",
    "\n",
    "\n",
    "_epoch = 1\n",
    "while _epoch <= _max_epoch :\n",
    "    \n",
    "    _model.fit_generator(_dataset_training,\n",
    "                         steps_per_epoch = _nb_batch_training,\n",
    "                         validation_data = _dataset_validation,\n",
    "                         epochs = 1,\n",
    "                         callbacks = [CUSTOM_CALLBACK])\n",
    "    \n",
    "    _classes = _dataset_testing.classes\n",
    "    _results = _model.predict_generator(_dataset_testing)\n",
    "    \n",
    "    _acc, _pod, _csi, _far, _hss, _tss = STATISTICS(_classes, _results)\n",
    "    _showing = '%5d'%_epoch + '%7.4f'%_acc + '%7.4f'%_pod + '%7.4f'%_csi + '%7.4f'%_far + '%7.4f'%_hss + '%7.4f'%_tss\n",
    "    print(_showing)\n",
    "    \n",
    "    _epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "FIG, LOSS_AX = plt.subplots()\n",
    "ACC_AX = LOSS_AX.twinx()\n",
    "LOSS_AX.plot(CUSTOM_HIST.train_loss, 'y', label = 'TRAIN LOSS')\n",
    "LOSS_AX.plot(CUSTOM_HIST.val_loss, 'r', label = 'VAL LOSS')\n",
    "ACC_AX.plot(CUSTOM_HIST.train_acc, 'b', label = 'TRAIN ACC')\n",
    "ACC_AX.plot(CUSTOM_HIST.val_acc, 'g', label = 'VAL ACC')\n",
    "LOSS_AX.set_xlabel('EPOCH')\n",
    "LOSS_AX.set_ylabel('LOSS')\n",
    "ACC_AX.set_ylabel('ACCURACY')\n",
    "LOSS_AX.legend(loc='upper left')\n",
    "ACC_AX.legend(loc='lower left')\n",
    "#FIG.savefig('./' + NAME_MODEL + '.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
